# -*- coding: utf-8 -*-
"""zadanie2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wUoFec4NLECskxEvjg9CK-8OxVq44jpV
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import datetime

from google.colab import drive
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import plot_tree
from yellowbrick.regressor import ResidualsPlot
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import r2_score,make_scorer, mean_squared_error
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

drive.mount('/content/drive')
data_path = '/content/drive/MyDrive/SUNS/zadanie2/'
train_data_path = data_path + 'train_dummy.csv'
test_data_path = data_path + 'test_dummy.csv'

df = pd.read_csv(train_data_path)
dft = pd.read_csv(test_data_path)

x_train = df.drop("SalePrice", axis=1)
y_train = df["SalePrice"]

x_test = dft.drop("SalePrice", axis=1)
y_test = dft["SalePrice"]

x_train

scaler = MinMaxScaler();
scaler.fit(x_train)
x_train = scaler.transform(x_train)
x_test = scaler.transform(x_test)

x_train

tree = DecisionTreeRegressor(max_depth=4, min_samples_leaf=50)
tree.fit(x_train,y_train)
plt.figure(dpi=500, figsize=(50,25))
plot_tree(tree)

print('tree')

tree = DecisionTreeRegressor()
params = {
    "max_depth": range(2,10),
    "min_samples_leaf": range(2,10),
    "criterion":["squared_error", "friedman_mse", "absolute_error", "poisson"]
}

grid_search = GridSearchCV(tree, params, scoring = make_scorer(r2_score))
grid_search.fit(x_train, y_train)

grid_search.best_params_

r2_score(y_test, grid_search.best_estimator_.predict(x_test))

mean_squared_error(y_test, grid_search.best_estimator_.predict(x_test))

# graf pre rezidualy pri decision tree
# scikit-yb.org/en/latest/api/regressor/residuals.html?fbclid=IwAR3b6NoZcOjRJjMGce5U-XPOsJwh4Jh5wAC-ikgm4S8UWR8HxIq68LHU77o
visualizer = ResidualsPlot(grid_search)

visualizer.fit(x_train, y_train)  
visualizer.score(x_test, y_test)  
visualizer.show()

# svm

params = {
    "kernel": ["rbf", "linear"],
    "gamma": [1000, 100, 10, 1, 0.1, 0.01, ],
    "C": [10, 100, 1000, 10000, 100000]
}

svm = GridSearchCV(SVR(), params, n_jobs = -1, verbose=1)
svm.fit(x_train, y_train)
svm.best_params_

r2_score(y_test, svm.best_estimator_.predict(x_test))

mean_squared_error(y_test, svm.best_estimator_.predict(x_test))

# scikit-yb.org/en/latest/api/regressor/residuals.html?fbclid=IwAR3b6NoZcOjRJjMGce5U-XPOsJwh4Jh5wAC-ikgm4S8UWR8HxIq68LHU77o
visualizer = ResidualsPlot(svm)

visualizer.fit(x_train, y_train)  
visualizer.score(x_test, y_test)  
visualizer.show()

# random forest regresor

params = {
    "criterion": ["squared_error", "absolute_error", "poisson"],
    "n_estimators": range(10,200,20),
    "max_features": ["sqrt","log2"],
    "max_depth": range(1,10)
}

forest = RandomForestRegressor()
grid_search = GridSearchCV(forest, params, cv=2, n_jobs=-1)
grid_search.fit(x_train, y_train)
grid_search.best_params_

r2_score(y_test, grid_search.best_estimator_.predict(x_test))

mean_squared_error(y_test, grid_search.best_estimator_.predict(x_test))

# scikit-yb.org/en/latest/api/regressor/residuals.html?fbclid=IwAR3b6NoZcOjRJjMGce5U-XPOsJwh4Jh5wAC-ikgm4S8UWR8HxIq68LHU77o
visualizer = ResidualsPlot(grid_search)

visualizer.fit(x_train, y_train)  
visualizer.score(x_test, y_test)  
visualizer.show()

#PCA
# https://stackoverflow.com/questions/65241847/how-to-plot-3d-pca-with-different-colors

fig = plt.figure(figsize=(15,10), dpi=500)
ax = fig.add_subplot(111, projection='3d')
fig.patch.set_facecolor('white')
ax.scatter(df["YearBuilt"], df["LotArea"], df["YrSold"], c=df["SalePrice"], s=40, cmap='viridis')
ax.set_xlabel("YearBuilt")
ax.set_ylabel('LotArea')
ax.set_zlabel('YrSold')

# https://stackoverflow.com/questions/65241847/how-to-plot-3d-pca-with-different-colors

pca = PCA(n_components=3)
pca.fit(x_train)
X_pca = pca.transform(x_train) 

Xax = X_pca[:,0]
Yax = X_pca[:,1]
Zax = X_pca[:,2]

fig = plt.figure(figsize=(15,10), dpi=500)
ax = fig.add_subplot(111, projection='3d')

fig.patch.set_facecolor('white')
ax.scatter(Xax, Yax, Zax, c=y_train, s=40, cmap='viridis')

ax.set_xlabel("pca1", fontsize=14)
ax.set_ylabel("pca2", fontsize=14)
ax.set_zlabel("pca3", fontsize=14)

plt.show()

params = {
    "kernel": ["rbf", "linear"],
    "gamma": [1000, 100, 10, 1, 0.1, 0.01, 0.001],
    "C": [10, 100, 1000, 10000, 100000, 1000000]
}

svm = GridSearchCV(SVR(), params, n_jobs = -1, verbose=4)

plot_values = {
    "x": [],
    "r2": [],
    "time": []
}

def reduce_dimensions(x):
  print("dimenzie:",x)
  pca = PCA(n_components=x)
  pca.fit(x_train)
  X_pca = pca.transform(x_train) 
  X_pca_test = pca.transform(x_test)
  start = datetime.datetime.now()
  svm.fit(X_pca, y_train)
  end = datetime.datetime.now()
  time = end - start
  r2 = r2_score(y_test, svm.best_estimator_.predict(X_pca_test))
  plot_values["x"].append(x) 
  plot_values["r2"].append(r2) 
  plot_values["time"].append(time.seconds) 
  print(svm.best_params_)
  print(r2)
  print(mean_squared_error(y_test, svm.best_estimator_.predict(X_pca_test)))
  print(time)
  visualizer = ResidualsPlot(svm)
  visualizer.fit(X_pca, y_train)  
  visualizer.score(X_pca_test, y_test)  
  visualizer.show()

reduce_dimensions(3)
reduce_dimensions(4)
reduce_dimensions(5)
reduce_dimensions(6)
reduce_dimensions(7)
reduce_dimensions(10)
reduce_dimensions(20)
reduce_dimensions(50)

plot_values = pd.DataFrame(plot_values)

# https://cmdlinetips.com/2019/10/how-to-make-a-plot-with-two-different-y-axis-in-python-with-matplotlib/
fig,ax=plt.subplots()
ax.plot(plot_values.x, plot_values.r2, marker="o")
ax.set_xlabel("x")
ax.set_ylabel("r2")
# ax.plot(plot_values.x, plot_values["time"], marker="o")
plt.show()

# https://cmdlinetips.com/2019/10/how-to-make-a-plot-with-two-different-y-axis-in-python-with-matplotlib/
fig,ax=plt.subplots()
ax.plot(plot_values.x, plot_values.time, marker="o")
ax.set_xlabel("x")
ax.set_ylabel("time")
plt.show()

# Clustering
# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html

x_train = df.drop("SalePrice", axis=1)
y_train = df["SalePrice"]

x_test = dft.drop("SalePrice", axis=1)
y_test = dft["SalePrice"]

scaler = MinMaxScaler();
scaler.fit(x_train)
x_train[x_train.columns] = scaler.transform(x_train[x_train.columns])
x_test[x_test.columns] = scaler.transform(x_test[x_test.columns])

kmeans = KMeans(n_clusters= 5)
kmeans.fit(x_train[["YearBuilt", "LotArea", "YrSold"]])
x_train["kmeans"] = kmeans.labels_

# https://stackoverflow.com/questions/65241847/how-to-plot-3d-pca-with-different-colors

fig = plt.figure(figsize=(15,10), dpi=500)
ax = fig.add_subplot(111, projection='3d')
fig.patch.set_facecolor('white')
ax.scatter(x_train["YearBuilt"], x_train["LotArea"], x_train["YrSold"], c=x_train["kmeans"], s=40, cmap='viridis')
ax.set_xlabel("YearBuilt")
ax.set_ylabel('LotArea')
ax.set_zlabel('YrSold')

kmeans = KMeans(n_clusters= 5)
kmeans.fit(x_train[["YearBuilt", "1stFlrSF", "2ndFlrSF"]])
x_train["kmeans"] = kmeans.labels_

# https://stackoverflow.com/questions/65241847/how-to-plot-3d-pca-with-different-colors

fig = plt.figure(figsize=(15,10), dpi=500)
ax = fig.add_subplot(111, projection='3d')
fig.patch.set_facecolor('white')
ax.scatter(x_train["YearBuilt"], x_train["1stFlrSF"], x_train["2ndFlrSF"], c=x_train["kmeans"], s=40, cmap='viridis')
ax.set_xlabel("YearBuilt")
ax.set_ylabel('1stFlrSF')
ax.set_zlabel('2ndFlrSF')

kmeans = KMeans(n_clusters= 5)
kmeans.fit(x_train[["YearBuilt", "GarageCars", "GarageArea"]])
x_train["kmeans"] = kmeans.labels_

# https://stackoverflow.com/questions/65241847/how-to-plot-3d-pca-with-different-colors

fig = plt.figure(figsize=(15,10), dpi=500)
ax = fig.add_subplot(111, projection='3d')
fig.patch.set_facecolor('white')
ax.scatter(x_train["YearBuilt"], x_train["GarageCars"], x_train["GarageArea"], c=x_train["kmeans"], s=40, cmap='viridis')
ax.set_xlabel("YearBuilt")
ax.set_ylabel('GarageCars')
ax.set_zlabel('GarageArea')